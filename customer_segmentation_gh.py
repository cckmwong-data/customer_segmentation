# -*- coding: utf-8 -*-
"""Customer_Segmentation_gh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDt04VnCsKOEMoTvxs4YYnVLeojRQ8d-

#**CUSTOMER SEGMENTATION USING K-PROTOTYPE**

# (1) Import Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, normalize, OneHotEncoder
from kmodes.kprototypes import KPrototypes
from sklearn.decomposition import PCA
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from pandas_gbq import read_gbq
import gower
from sklearn.metrics import silhouette_score

import random

# Fix global seeds
np.random.seed(42)
random.seed(42)

import os

# read environment variable when running Github Action where we defined RUN_GENAI as false
# not run Gen AI block if checked RUN_GENAI is false
RUN_GENAI = os.getenv("RUN_GENAI", "false").lower() == "true"
print("RUN_GENAI =", RUN_GENAI)

"""# (2) Import Dataset
First, we collect customer, sales and product data from the three spreadsheets stored in Google Sheet, namely 1) customers.csv, 2) sales.csv and 3) products.csv, respectively.
"""

# Extract the customer data from a csv file in Google Sheet
df_customers = pd.read_csv('https://docs.google.com/spreadsheets/d/1bzB4VDQEQz0BIbyT2BCIvcE2rPSAlOcmeQuvMtuIFcI/export?format=csv')
df_customers

# Extract the sales data from a csv file in Google Sheet
df_sales = pd.read_csv('https://docs.google.com/spreadsheets/d/1xrPtoy8R965CPSCS59_hcLP1cwzNcp7DJ2MOegZrsVA/export?format=csv')
df_sales

# Extract the products data from a csv file in Google Sheet
df_products = pd.read_csv('https://docs.google.com/spreadsheets/d/1HpCKgERS0F9qjHh3y0_GZ988JjX4vtKBX5rIksCgB5I/gviz/tq?tqx=out:csv&gid=0')
df_products

"""# (3) Data Cleaning
This section performs an initial exploratory check on the three datasets — customers, sales, and products — to understand their structure and data quality. We inspect the columns, data types, and non-null counts for each dataset, ensuring that all expected fields are present and correctly formatted. Then we check for missing values and any duplicate rows. This process provides a quick overview of the data completeness and consistency before proceeding to further preprocessing or analysis.
"""

# Check the columns and datatypes of customers data
df_customers.info()

# Check the columns and datatypes of sales data
df_sales.info()

# Check the columns and datatypes of products data
df_products.info()

# Check for missing values for customers data
df_customers.isnull().sum()

# Check for missing values for sales data
df_sales.isnull().sum()

# Check for missing values for products data
df_products.isnull().sum()

# Check if any duplicated rows
df_customers.duplicated().sum()

# Check if any duplicated rows
df_sales.duplicated().sum()

# Check if any duplicated rows
df_products.duplicated().sum()

"""# (4) Data Wraggling and Visualization
We further derive key metrics from the existing data, including customer loyalty (represented by the *Tenure_Years* column) and insurer profitability (captured through the *Premium_to_Coverage* ratio). These metrics provide insight into how long customers have been with the company and how much value they generate relative to their coverage. After computing these measures, we explored the distribution of both categorical and numerical variables to better understand customer demographics, purchasing behavior, and overall trends within the dataset.
"""

# Find the loyalty of the customers by calculating the number of years the customers purchased the policy

# Convert 'Purchase_History' to pandas datetime format
df_sales['Purchase_Date'] = pd.to_datetime(df_sales['Purchase_Date'], format='mixed')

# Get today's date as a pandas Timestamp
today = pd.Timestamp.today()

# Convert 'Purchase_History' to individual timestamps before subtracting
df_sales['Tenure_Years'] = (today - df_sales['Purchase_Date'].astype('datetime64[ns]')).dt.days / 365

df_sales['Tenure_Years'] = df_sales['Tenure_Years'].round(2)

# Calculate premium to coverage ratio which shows the profitability for insurance company
# Higher the ratio, higher the amount customers willing to pay their premiums
# relative to the amount of coverage they are receiving

df_sales['Premium_to_Coverage'] = df_sales['Premium']/ df_sales['Coverage']*100
df_sales

# merge the sales and customers data for clustering
df_merged = pd.merge(df_sales, df_customers, on='Customer_ID', how='left')
df_merged

# group the data by customer and average the metrics for each customer
df_grouped_customer = df_merged.groupby(['Customer_ID', 'Marital_Status']).agg(
                      Age = ('Age', 'mean'),
                      Income = ('Income', 'mean'),
                      Tenure_Years = ('Tenure_Years', 'mean'),
                      Premium_to_Coverage = ('Premium_to_Coverage',  'mean')
)

df_grouped_customer = df_grouped_customer.reset_index() # reset the index to free the columns
df_grouped_customer

# Extract the columns that are useful for the analysis
df = df_grouped_customer[['Age', 'Marital_Status','Income','Tenure_Years', 'Premium_to_Coverage']].copy()

df

# Identify the columns with categorical and numeric labels
categorical_cols = ['Marital_Status']
numerical_cols = ['Age', 'Income', 'Tenure_Years', 'Premium_to_Coverage']

# Define categorical column indices
categorical_indices = [df.columns.get_loc(col) for col in categorical_cols]
categorical_indices

# Define categorical column indices
numeric_indices = [df.columns.get_loc(col) for col in numerical_cols]
numeric_indices

# Shows the distribution of each feature

plt.figure(figsize=(12, 24))
colours_val = ['c', 'b', 'r', 'm']

length = len(categorical_indices) + len(numerical_cols)

# Pie Charts for Categorical Features
for j, i in enumerate(categorical_indices):
    plt.subplot(length, 2, j + 1)
    value_counts = df.iloc[:, i].value_counts()
    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%',
            colors=sns.color_palette("pastel"), startangle=140)
    plt.title(df.columns[i])

# Histograms for Numerical Features
for j, i in enumerate(numeric_indices):
    plt.subplot(length, 2, len(categorical_indices) + j + 1)
    sns.histplot(df.iloc[:, i], kde=True, color=colours_val[j])
    plt.title(df.columns[i])

plt.tight_layout()
plt.show()

"""# (5) Feature Engineering

Due to a mix of categorical and numeric data, we prefer K-Prototype to K-means for the segmentation, as the latter only works for numerical data. Before data clustering, we may need to transform raw data into features which can be used in the model.

*   *Categorical labels*: These can be used directly in the K-Prototypes algorithm without any conversion, as it is designed to handle categorical features natively through matching dissimilarities.

*   *Numerical labels*: Scale the numerical features to a range of 0 to 1 using **MinMaxScaler**. We prefer MinMaxScaler over StandardScaler because it bounds all values within the same range, ensuring that each numerical feature contributes equally to the overall distance calculation in this distance-based segmentation algorithm.


"""

df_c = df.copy()

# ensure all columns with numerical values are in float data type
for col in numerical_cols:
    df_c[col] = df_c[col].astype('float64')

# Statistics of the dataset
df_c.describe()

# scale the numerical variables
scaler = MinMaxScaler()
df_c[numerical_cols] = scaler.fit_transform(df_c[numerical_cols])
df_c

# convert the data to array for clustering in the next step
c_array = df_c.to_numpy()
c_array

"""# (6) Clustering using K-Prototypes

K-prototypes combines the use of minimizing Euclidean distance for numerical labels and comparing matching dissimilarity for categorical labels, to determine the cluster centroids. We locate the optimal k (the no. of clusters) using Elbow Method, where the optimal k is the point that the rate of decrease in **WCSS (Within-Cluster Sum of Squares)** slows down or flattens. WCSS measures the total variance within each cluster.
"""

# we set k as the number of clusters

costs = {} # a dictionary to store the WCSS (Within-Cluster Sum of Square) for each k
labels = {} # a dictionary to store the predicted cluster labels for each k

range_of_k = range(2,10) # we set k from 2 to 9
for k in range_of_k :
    print("Checking k =", k)
    untrained_model = KPrototypes(n_clusters=k, init='Cao', max_iter=20, random_state=42, verbose=1) # fixed random seed
    trained_model = untrained_model.fit(c_array, categorical=categorical_indices)

    labels[k] = trained_model.labels_ # predicted labels
    costs[k]=trained_model.cost_ # WCSS

# We hope to minimize the cost (i.e. WCSS)
# When k = 3, the elbow curve flattens out significantly.
# Hence, we choose the optimal no. of clustering to be 3.
plt.plot(costs.keys(), costs.values())
plt.scatter(costs.keys(),costs.values())
plt.xlabel("Values of K")
plt.ylabel("Cost (WCSS)")
plt.title("Elbow Curve")
plt.show()

"""# (7) Evaluation of the Model
The model’s performance is evaluated using the **Silhouette Score**. To further assess the quality of the segmentation, we visualize the clustering results in two dimensions using **Factor Analysis of Mixed Data (FAMD)** — a dimensionality reduction technique suitable for datasets containing both categorical and numerical variables. The resulting plot reveals well-defined and distinct cluster structures.
"""

# Copy the predictions for k = 3 which we found from the WCSS chart
pred = labels[3]
customers_clustered = df.copy()
customers_clustered['Cluster'] = pred

# Compute the custom distance matrix on the sampled data to be used for calculating the Silhouette Score
# Gower distance is to measure similarity between data points
gower_dist = gower.gower_matrix(customers_clustered.drop(columns=['Cluster']))

# Extract cluster labels for calculating the Silhouette Score
cluster_labels = customers_clustered['Cluster']

# Check the Silhouette Score for k = 3
# A Silhouette Score of 0.43 shows a moderate clustering quality
sil_score = silhouette_score(gower_dist, cluster_labels, metric='precomputed')
print('Silhouette Score: ', sil_score)

df_c # dataframe with scaled numerical data and categorical labels

# reduced the dimensionality to 2-D for visualing the clustering
from prince import FAMD

famd = FAMD(n_components=2, random_state=42)
X_famd = famd.fit_transform(df_c)

X_famd #2-D

# Add cluster labels to the FAMD result for plotting
X_famd['Cluster'] = pred

# Scatter plot of the 2D data
plt.figure(figsize=(10, 5))
sns.scatterplot(
    data = X_famd,
    x = X_famd.columns[0],
    y = X_famd.columns[1],
    hue ='Cluster',
    palette ='tab10',
    s = 20,
    alpha = 0.6,
)

plt.title("2-D Plot with FAMD")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.tight_layout()
plt.show()

"""# (8) Analysis of the Customer Segments
After clustering the customers into three distinct segments, we analyze each group’s demographic and policy-related characteristics to better understand their unique profiles. We then use Gemini AI to generate descriptive cluster labels and segment descriptions by providing the summarized metrics of each segment, with the Gemini API key securely stored in Colab’s secret manager.
"""

customers_clustered

#from google.colab import userdata
import google.genai as genai

# Try Colab userdata
try:
    from google.colab import userdata
    api_key = userdata.get('GEMINI_API_KEY')
except Exception:
    # Fallback: use environment variable (GitHub Actions, local)
    api_key = os.getenv("GEMINI_API_KEY")

client = genai.Client(api_key=api_key) # Create the Gemini client

# Summarize the features for each cluster
customers_clustered.groupby('Cluster').agg({
    "Age": "mean",
    "Marital_Status": lambda x: x.value_counts().index[0], # mode of marital status
    "Income": "mean",
    "Tenure_Years": "mean",
    "Premium_to_Coverage": "mean",
})

# rename the columns
cluster_summary = (
    customers_clustered
    .groupby('Cluster')
    .agg({
        "Age": "mean",
        "Marital_Status": lambda x: x.value_counts().index[0],
        "Income": "mean",
        "Tenure_Years": "mean",
        "Premium_to_Coverage": "mean",
    })
    .rename(columns={
        "Age": "Age_avg",
        "Marital_Status": "Marital_Status_mode",
        "Income": "Income_avg",
        "Tenure_Years": "Tenure_Years_avg",
        "Premium_to_Coverage": "Premium_to_Coverage_avg",
    })
    .reset_index()
)

cluster_summary

# Due to the randomness of cluster id, sort clusters by Age_avg
cluster_summary_sorted = cluster_summary.sort_values(
    ["Age_avg"], ascending=[True]
).reset_index(drop=True)

# Assign deterministic IDs
cluster_summary_sorted["Cluster_ID"] = range(len(cluster_summary_sorted))
cluster_summary_sorted

# Create mapping rule for converting old cluster IDs to new ones
id_map = dict(
    zip(cluster_summary_sorted["Cluster"], cluster_summary_sorted["Cluster_ID"])
)

id_map

# Drop the old cluster ID, replaced by new cluster ID based on the mean age
cluster_summary_sorted = cluster_summary_sorted.drop(['Cluster'], axis=1)
cluster_summary_sorted

# Apply mapping rule to convert old cluster IDs to new IDs in the customer dataset
customers_clustered["Cluster_ID"] = customers_clustered["Cluster"].map(id_map)

customers_clustered = customers_clustered.drop(columns=['Cluster'])

customers_clustered

# Function for asking Gemini for the cluster names and descriptions
import json
import re

def get_cluster_labels_from_gemini(summary_df):
    # Turn the cluster summary into a list of dicts
    data = summary_df.to_dict(orient="records")

    prompt = f"""
You are a marketing analyst for an insurance company.

You will receive a list of customer clusters with summary statistics.
Each record has:
- Cluster: internal numeric ID
- Age_avg
- Marital_Status_mode
- Income_avg
- Tenure_Years_avg
- Premium_to_Coverage_avg

Task:
1. For each cluster, create:
   - name: a short, catchy 2–4 word segment name
   - description: one concise sentence explaining the segment (max 25 words)

2. Respond ONLY with valid JSON in this exact format:

{{
  "0": {{
    "cluster_name": "Segment Name",
    "description": "Short sentence..."
  }},
  "1": {{
    "cluster_name": "Another Name",
    "description": "Short sentence..."
  }}
}}

Here is the cluster data (as JSON):
{json.dumps(data, indent=2)}
"""

    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt
    )

    text = response.text.strip()

    # Strip code fences if Gemini wraps the JSON in ```json ... ```
    text = re.sub(r"^```json", "", text, flags=re.IGNORECASE).strip()
    text = re.sub(r"^```", "", text).strip()
    text = re.sub(r"```$", "", text).strip()

    labels_json = json.loads(text)
    return labels_json

# Feeding the summarized data of each segment into Gemini
# RUN_GENAI = True

if RUN_GENAI:
    print("Running Gen-AI labeling...")
    labels_json = get_cluster_labels_from_gemini(cluster_summary_sorted)
else:
    print("Skipping Gen-AI — loading labels from BigQuery.")

    df_labels = read_gbq(
        "SELECT cluster_id, cluster_name, description FROM portfolio.df_labels",
        project_id="new-project-456705"
    )

    labels_json = {
        str(row["cluster_id"]): {
            "cluster_name": row["cluster_name"],
            "description": row["description"]
        }
        for _, row in df_labels.iterrows()
    }

# Summary of the cluster names and their descriptions
df_labels = pd.DataFrame.from_dict(labels_json, orient='index').reset_index()
df_labels = df_labels.rename(columns={'index': 'cluster_id'})
df_labels

# Map cluster name and descriptions to the customer table
cluster_name_map = {int(k): v["cluster_name"] for k, v in labels_json.items()}
cluster_desc_map = {int(k): v["description"] for k, v in labels_json.items()}

customers_clustered["Cluster_Label"] = customers_clustered["Cluster_ID"].map(cluster_name_map)
customers_clustered["Cluster_Des"] = customers_clustered["Cluster_ID"].map(cluster_desc_map)

customers_clustered

# Show the proportion of customer in each segment
plt.figure(figsize=(8, 5))
value_counts = customers_clustered.iloc[:, -2].value_counts() # Counts how many customers belong to each cluster label
plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%',
        colors=sns.color_palette("pastel"), startangle=140)
plt.title("Proportion of Customers in Each Segment")
plt.show()

# Analyze the distribution of categorical features in each segment
for feature in categorical_cols:
    df_i = customers_clustered[[feature, 'Cluster_Label']].groupby(['Cluster_Label'])

    # Create consistent color map per feature
    categories = customers_clustered[feature].unique()
    palette = sns.color_palette("pastel", len(categories))
    color_map = dict(zip(categories, palette))

    # Create a new figure of subplots for each feature
    plt.figure(figsize=(18, 5))
    j = 1 # counter for the position of each subplot

    # Make a subplot for the feature distribution for each cluster
    for cluster in customers_clustered.Cluster_Label.unique():
      # Count the number of every unique items of a feature of a selected cluster
        df_c = df_i.get_group((cluster,))[feature] # extract one feature column for a selected cluster
        value_counts = df_c.value_counts() # count the number of every unique items within that feature
        # set the colours of the features
        colors = [color_map[val] for val in value_counts.index]

        plt.subplot(1, 3, j) # show 3 subplots in every row
        plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%',
                colors=colors, startangle=140)
        plt.title(f"{feature} in {cluster}")
        # move the counter by 1 for preparing making another subplot for the next cluster
        j = j + 1

    plt.suptitle(f"Distribution of {feature} Across Clusters", fontsize=16)
    plt.tight_layout()
    plt.show()


# Boxplot for numerical features
plt.figure(figsize=(14, 8))
j = 1 # counter for the position of subplot

for feature in numerical_cols:
    # Boxplot for numerical variable per cluster
    plt.subplot(2, 2, j)
    sns.boxplot(x = 'Cluster_Label', y = feature, data = customers_clustered)
    plt.title(f"Distribution of {feature} per Segment")
    j = j + 1

plt.tight_layout()
plt.show()

"""# (9) Load the Datasets to Google BigQuery
Finally, the updated customers, products, and sales tables are written directly to *Google BigQuery*, where the process is automated to run daily via a *GitHub Action* workflow.
"""

# Complete customer table with the sorted cluster ID
df_clustered_customers = pd.concat([df_customers, customers_clustered['Cluster_ID']], axis=1) # concatenate the columns of cluster number to the original dataset

df_clustered_customers

df_sales

df_products

X_famd # the cluster is based on the previous unsorted cluster id

# Map the old cluster ID to sorted cluster ID in X_famd
X_famd['Cluster_ID'] = X_famd['Cluster'].map(id_map)
X_famd = X_famd.drop(['Cluster'], axis = 1)

X_famd # based on the sorted new cluster_id

X_famd["Cluster_ID"] = X_famd["Cluster_ID"].astype(int) # convert to string for table merge
df_labels["cluster_id"] = df_labels["cluster_id"].astype(int) # convert to string for table merge

famd_chart = X_famd.merge(
    df_labels,
    how="left",
    left_on="Cluster_ID",
    right_on="cluster_id"
)

famd_chart = famd_chart.drop(['cluster_id'], axis = 1)

famd_chart

# converts all column names to strings
famd_chart.columns = famd_chart.columns.map(str)
famd_chart

# Import the to_gbq() function from the pandas_gbq package to write
# panda dataframe to Google BigQuery tables
from pandas_gbq import to_gbq

# Write the customer dataset to BigQuery
to_gbq(
    df_clustered_customers,
    destination_table='portfolio.customers',
    project_id='new-project-456705',
    if_exists='replace'
)

# Write the sales dataset to BigQuery
to_gbq(
    df_sales,
    destination_table='portfolio.sales',
    project_id='new-project-456705',
    if_exists='replace'
)

# Write the products dataset to BigQuery
to_gbq(
    df_products,
    destination_table='portfolio.products',
    project_id='new-project-456705',
    if_exists='replace'
)

# Write the data of the 2-D clustering plot to Bigqery
to_gbq(
    famd_chart,
    destination_table='portfolio.scatter_chart',
    project_id='new-project-456705',
    if_exists='replace'
)

# Write the data of the cluster labels table to Bigqery
to_gbq(
    df_labels,
    destination_table='portfolio.df_labels',
    project_id='new-project-456705',
    if_exists='replace'
)

# Write the data of the cluster summary to Bigqery
to_gbq(
    cluster_summary_sorted,
    destination_table='portfolio.cluster_summary_sorted',
    project_id='new-project-456705',
    if_exists='replace'
)